{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementing Proximal Policy Optimization \n",
    "\n",
    "\n",
    "In this notebook you will be implementing Proximal Policy Optimization algorithm, \n",
    "scaled up version of which was used to train [OpenAI Five](https://openai.com/blog/openai-five/) \n",
    "to [win](https://openai.com/blog/how-to-train-your-openai-five/) against the\n",
    "world champions in Dota 2.\n",
    "You will be solving a continuous control environment on which it may be easier and faster \n",
    "to train an agent, however note that PPO here may not be the best algorithm as, for example,\n",
    "Deep Deterministic Policy Gradient and Soft Actor Critic may be more suited \n",
    "for continuous control environments. To run the environment you will need to install \n",
    "[pybullet-gym](https://github.com/benelot/pybullet-gym) which unlike MuJoCo \n",
    "does not require you to have a license.\n",
    "\n",
    "**If you have some weird troubles with pybullet, try ver.2.5.6 : `pip install pybullet==2.5.6`**\n",
    "\n",
    "To install the library:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-15T15:24:37.890864Z",
     "start_time": "2020-09-15T15:24:36.895890Z"
    }
   },
   "outputs": [],
   "source": [
    "!git clone https://github.com/benelot/pybullet-gym lib/pybullet-gym\n",
    "!pip install -e lib/pybullet-gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym \n",
    "import pybulletgym"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The overall structure of the code is similar to the one in the A2C optional homework, but don't worry if you haven't done it, it should be relatively easy to figure it out. \n",
    "\n",
    "First, we will create an instance of the environment. The first wrapper will simply write summaries, mainly, the total reward during an episode. Then we will *normalize* the observations and rewards: subtract running mean from observations and rewards and divide the resulting quantities by the running variances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-16T18:41:00.003174Z",
     "start_time": "2020-09-16T18:40:59.921640Z"
    }
   },
   "outputs": [],
   "source": [
    "from mujoco_wrappers import Normalize\n",
    "from logger import TensorboardSummaries as Summaries\n",
    "\n",
    "env = gym.make(\"HalfCheetahMuJoCoEnv-v0\")\n",
    "env = Normalize(Summaries(env));\n",
    "env.unwrapped.seed(0);\n",
    "\n",
    "print(\"observation space: \", env.observation_space,\n",
    "      \"\\nobservations:\", env.reset())\n",
    "print(\"action space: \", env.action_space, \n",
    "      \"\\naction_sample: \", env.action_space.sample())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, you will need to define a model for training. You can use two separate networks (one for policy and another for value function) or create one with shared backbone. Recommended architecture is a 3-layer MLP with 64 hidden units, $\\mathrm{tanh}$ \n",
    "activation function, weights initialized with orthogonal initializer with gain $\\sqrt{2}$ and biases initialized with zeros. \n",
    "\n",
    "Our policy distribution is going to be multivariate normal with diagonal covariance. The policy head will predict the mean and covariance OR only mean, and the covariance then should be represented by a single (learned) vector of size 6 (corresponding to the dimensionality of the action space from above), initialized with zeros. Anyway you should guarantee that covariance is non-negative by using exponent or softplus. \n",
    "\n",
    "Overall the model should return three things: predicted mean of the distribution, variance vector and value estimation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-16T18:41:06.246418Z",
     "start_time": "2020-09-16T18:41:05.841255Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "<Define your model here>\n",
    "\n",
    "'''\n",
    "input:\n",
    "    states - tensor, (batch_size x features)\n",
    "output:\n",
    "    mean - tensor, (batch_size x actions_dim)\n",
    "    cov - tensor, (batch_size x actions_dim)\n",
    "    V - tensor, critic estimation, (batch_size)\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model will be wrapped by a `Policy`. The policy can work in two modes, but in either case \n",
    "it is going to return dictionary with string-type keys. The first mode is when the policy is \n",
    "used to sample actions for a trajectory which will later be used for training. In this case \n",
    "the flag `training` passed to `act` method is `False` and the method should return \n",
    "a `dict` with the following keys: \n",
    "\n",
    "* `\"actions\"`: actions to pass to the environment\n",
    "* `\"log_probs\"`: log-probabilities of sampled actions\n",
    "* `\"values\"`: value function $V^\\pi(s)$ predictions.\n",
    "\n",
    "We don't need to use the values under these keys for training, so all of them should be of type `np.ndarray`. This regime will be used to collect data.\n",
    "\n",
    "When `training` is `True`, the model is training on a given batch of observations. In this\n",
    "case it should return a `dict` with the following keys\n",
    "\n",
    "* `\"distribution\"`: an instance of multivariate normal distribution (`torch.distributions.MultivariateNormal`)\n",
    "* `\"values\"`: value function $V^\\pi(s)$ prediction, tensor\n",
    "\n",
    "The distinction about the modes comes into play depending on where the policy is used: if it is called from `EnvRunner`, \n",
    "the `training` flag is `False`, if it is called from `PPO`, the `training` flag is `True`. These classes \n",
    "will be described below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-16T18:41:09.977302Z",
     "start_time": "2020-09-16T18:41:09.938325Z"
    }
   },
   "outputs": [],
   "source": [
    "from torch.distributions import MultivariateNormal\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "class Policy:\n",
    "    def __init__(self, model):\n",
    "        self.model = model\n",
    "\n",
    "    def act(self, inputs, training=False):\n",
    "        '''\n",
    "        input:\n",
    "            inputs - numpy array if training is False, otherwise tensor, (batch_size x features)\n",
    "            training - flag, bool\n",
    "        output:\n",
    "            if training is True, dict containing keys ['actions', 'log_probs', 'values']:\n",
    "                `distribution` - MultivariateNormal, (batch_size x actions_dim)\n",
    "                'values' - critic estimations, tensor, (batch_size)\n",
    "            if training is False, dict containing keys ['actions', 'log_probs', 'values']:\n",
    "                'actions' - selected actions, numpy, (batch_size)\n",
    "                'log_probs' - log probs of selected actions, numpy, (batch_size)\n",
    "                'values' - critic estimations, numpy, (batch_size)\n",
    "        '''\n",
    "        # if training is false, input is numpy\n",
    "        if not training:\n",
    "            inputs = torch.FloatTensor(inputs).to(DEVICE)            \n",
    "        \n",
    "        <YOUR CODE>\n",
    "        \n",
    "        if training:\n",
    "            return {\n",
    "                \"distribution\": <>,\n",
    "                \"values\": <>\n",
    "            }\n",
    "        else:\n",
    "            return {\n",
    "                \"actions\": <>,\n",
    "                \"log_probs\": <>,\n",
    "                \"values\": <>\n",
    "            }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use `EnvRunner` to perform interactions with an environment with a policy for a fixed number of timesteps. Calling `.get_next()` on a runner will return a trajectory &mdash; dictionary \n",
    "containing keys\n",
    "\n",
    "* `\"observations\"`\n",
    "* `\"rewards\"` \n",
    "* `\"dones\"`\n",
    "* `\"actions\"`\n",
    "* all other keys that you defined in `Policy` in `training=False` regime,\n",
    "\n",
    "under each of these keys there is a `np.ndarray` of specified length $T$ &mdash; the size of partial trajectory. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-16T19:01:29.060091Z",
     "start_time": "2020-09-16T19:01:29.040121Z"
    }
   },
   "outputs": [],
   "source": [
    "from runners import EnvRunner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Additionally, before returning a trajectory this runner can apply a list of transformations. \n",
    "Each transformation is simply a callable that should modify passed trajectory in-place."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-16T19:01:38.244347Z",
     "start_time": "2020-09-16T19:01:38.225359Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class AsArray:\n",
    "    \"\"\" \n",
    "    Converts lists of interactions to ndarray.\n",
    "    \"\"\"\n",
    "    def __call__(self, trajectory, last_observation):\n",
    "        # Modifies trajectory inplace.\n",
    "        # Just switches python lists to numpy arrays\n",
    "        for k, v in trajectory.items():\n",
    "            trajectory[k] = np.asarray(v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a look at how this works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-16T18:41:16.042000Z",
     "start_time": "2020-09-16T18:41:14.697937Z"
    }
   },
   "outputs": [],
   "source": [
    "model = <init your model>\n",
    "policy = Policy(model)\n",
    "runner = EnvRunner(env, policy, nsteps=5, transforms=[AsArray()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-16T18:41:16.521675Z",
     "start_time": "2020-09-16T18:41:16.043008Z"
    }
   },
   "outputs": [],
   "source": [
    "# generates new rollout\n",
    "trajectory = runner.get_next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-16T18:41:16.537666Z",
     "start_time": "2020-09-16T18:41:16.522643Z"
    }
   },
   "outputs": [],
   "source": [
    "# what is inside\n",
    "print(trajectory.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-16T18:41:16.633684Z",
     "start_time": "2020-09-16T18:41:16.538634Z"
    }
   },
   "outputs": [],
   "source": [
    "# Sanity checks\n",
    "assert 'log_probs' in trajectory, \"Not found: policy didn't provide log_probs of selected actions\"\n",
    "assert 'values' in trajectory, \"Not found: policy didn't provide critic estimations\"\n",
    "assert trajectory['log_probs'].shape == (5,), \"log_probs wrong shape\"\n",
    "assert trajectory['values'].shape == (5,), \"values wrong shape\"\n",
    "assert trajectory['observations'].shape == (5, 17), \"observations wrong shape\"\n",
    "assert trajectory['rewards'].shape == (5,), \"rewards wrong shape\"\n",
    "assert trajectory['dones'].shape == (5,), \"dones wrong shape\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-16T18:41:16.728696Z",
     "start_time": "2020-09-16T18:41:16.634676Z"
    }
   },
   "outputs": [],
   "source": [
    "# Here is what collected inside\n",
    "{k: v.shape for k, v in trajectory.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will need to implement the following two transformations. \n",
    "\n",
    "The first is `GAE` that implements [Generalized Advantage Estimator](https://arxiv.org/abs/1506.02438).\n",
    "In it you should add two keys to the trajectory: `\"advantages\"` and `\"value_targets\"`. In GAE the advantages\n",
    "$A_t^{\\mathrm{GAE}(\\gamma,\\lambda)}$ are essentially defined as the exponential \n",
    "moving average with parameter $\\lambda$ of the regular advantages \n",
    "$\\hat{A}^{(T)}(s_t) = \\sum_{l=0}^{T-1-t} \\gamma^l r_{t+l} + \\gamma^{T} V^\\pi(s_{T}) - V^\\pi(s_t)$. \n",
    "The exact formula for the computation is the following\n",
    "\n",
    "$$\n",
    "A_{t}^{\\mathrm{GAE}(\\gamma,\\lambda)} = \\sum_{l=0}^{T-1-t} (\\gamma\\lambda)^l\\delta_{t + l}^V, \\, t \\in [0, T)\n",
    "$$\n",
    "where $\\delta_{t+l}^V = r_{t+l} + \\gamma V^\\pi(s_{t+l+1}) - V^\\pi(s_{t+l})$. You can look at the \n",
    "derivation (formulas 11-16) in the paper. Don't forget to reset the summation on terminal\n",
    "states as determined by the flags `trajectory[\"dones\"]`. You can use `trajectory[\"values\"]`\n",
    "to get values of all observations except the most recent which is stored under \n",
    " `trajectory[\"state\"][\"latest_observation\"]`. For this observation you will need to call the policy \n",
    " to get the value prediction.\n",
    "\n",
    "Once you computed the advantages, you can get the targets for training the value function by adding \n",
    "back values:\n",
    "$$\n",
    "\\hat{V}(s_{t+l}) = A_{t+l}^{\\mathrm{GAE}(\\gamma,\\lambda)} + V(s_{t + l}),\n",
    "$$\n",
    "where $\\hat{V}$ is a tensor of value targets that are used to train the value function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-16T19:00:55.733373Z",
     "start_time": "2020-09-16T19:00:55.719400Z"
    }
   },
   "outputs": [],
   "source": [
    "class GAE:\n",
    "    \"\"\" Generalized Advantage Estimator. \"\"\"\n",
    "    def __init__(self, policy, gamma=0.99, lambda_=0.95):\n",
    "        self.policy = policy\n",
    "        self.gamma = gamma\n",
    "        self.lambda_ = lambda_\n",
    "\n",
    "    def __call__(self, trajectory, last_observation):\n",
    "        '''\n",
    "        This method should modify trajectory inplace by adding \n",
    "        items with keys 'advantages' and 'value_targets' to it\n",
    "        \n",
    "        input:\n",
    "            trajectory - dict from runner\n",
    "            latest_observation - last state, numpy, (features)\n",
    "        '''\n",
    "        <YOUR CODE>\n",
    "            \n",
    "        trajectory[\"advantages\"] = <>\n",
    "        trajectory[\"value_targets\"] = <>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's run a small test just in case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-16T19:01:43.829123Z",
     "start_time": "2020-09-16T19:01:43.817122Z"
    }
   },
   "outputs": [],
   "source": [
    "# tests\n",
    "class DummyEnv():\n",
    "    def __init__(self):\n",
    "        self.unwrapped = None\n",
    "        self.t = 0\n",
    "        self.state = np.zeros(17)\n",
    "    \n",
    "    def reset(self):\n",
    "        return self.state\n",
    "    \n",
    "    def step(self, a):\n",
    "        r = [0, -100, 800][self.t]\n",
    "        done = self.t == 2\n",
    "        self.t = (self.t + 1) % 3\n",
    "        return self.state, r, done, {}\n",
    "    \n",
    "class DummyPolicy():\n",
    "    def act(self, s):\n",
    "        return {\"values\": np.array(100), \"actions\": np.array([-0.42, 0.42])}\n",
    "\n",
    "dummy_env = DummyEnv()\n",
    "dummy_policy = DummyPolicy()\n",
    "runner = EnvRunner(dummy_env, dummy_policy, nsteps=8, transforms=[AsArray(), GAE(dummy_policy, gamma=0.8, lambda_=0.5)])\n",
    "trajectory = runner.get_next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-16T18:41:17.106319Z",
     "start_time": "2020-09-16T18:41:17.014164Z"
    }
   },
   "outputs": [],
   "source": [
    "assert 'advantages' in trajectory, \"Not found: advantage estimation\"\n",
    "assert 'value_targets' in trajectory, \"Not found: targets for critic\"\n",
    "assert trajectory['advantages'].shape == (8,), \"advantage wrong shape\"\n",
    "assert trajectory['value_targets'].shape == (8,), \"value_targets wrong shape\"\n",
    "assert (trajectory['advantages'] == np.array([44, 160, 700, 44, 160, 700, -68, -120])).all(), \"advantage computation error\"\n",
    "assert (trajectory['value_targets'] == trajectory['advantages'] + 100).all(), \"value targets computation error\"\n",
    "print(\"Nice!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main advantage of PPO over simpler policy based methods like A2C is that it is possible\n",
    "to train on the same trajectory for multiple gradient steps. The following class wraps \n",
    "an `EnvRunner`. It should call the runner to get a trajectory, then return minibatches \n",
    "from it for a number of epochs, shuffling the data before each epoch. The number of minibatches per epoch is predefined in `num_minibatches`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-16T18:41:17.294210Z",
     "start_time": "2020-09-16T18:41:17.274240Z"
    }
   },
   "outputs": [],
   "source": [
    "class Sampler:\n",
    "    \"\"\" Samples minibatches from trajectory for a number of epochs. \"\"\"\n",
    "    def __init__(self, runner, num_epochs, num_minibatches, transforms=None):\n",
    "        self.runner = runner\n",
    "        self.num_epochs = num_epochs\n",
    "        self.num_minibatches = num_minibatches\n",
    "        self.transforms = transforms or []\n",
    "\n",
    "    def get_next(self):\n",
    "        \"\"\" \n",
    "        Yields next minibatch (dict) for training with at least following keys:\n",
    "                'observations' - numpy, (batch_size x features)\n",
    "                'actions' - numpy, (batch_size x actions_dim)\n",
    "                'advantages' - numpy, (batch_size)\n",
    "                'log_probs' - numpy, (batch_size)\n",
    "        \"\"\"\n",
    "        trajectory = self.runner.get_next()\n",
    "        \n",
    "        for epoch in range(self.num_epochs):\n",
    "            # shuffle dataset and separate it into minibatches\n",
    "            # you can use any standard utils to do that\n",
    "            <YOUR CODE>\n",
    "            \n",
    "            for _ in range(self.num_minibatches):\n",
    "                <YOUR CODE>\n",
    "                \n",
    "                # applying additional transforms\n",
    "                for transform in self.transforms:\n",
    "                    transform(minibatch)\n",
    "                \n",
    "                yield minibatch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A common trick to use with GAE is to normalize advantages for every minibatch, the following transformation does that. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-16T18:41:17.842105Z",
     "start_time": "2020-09-16T18:41:17.825115Z"
    }
   },
   "outputs": [],
   "source": [
    "class NormalizeAdvantages:\n",
    "    \"\"\" Normalizes advantages to have zero mean and variance 1. \"\"\"\n",
    "    def __call__(self, batch):\n",
    "        adv = batch[\"advantages\"]\n",
    "        adv = (adv - adv.mean()) / (adv.std() + 1e-8)\n",
    "        batch[\"advantages\"] = adv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-16T18:41:18.094229Z",
     "start_time": "2020-09-16T18:41:18.083229Z"
    }
   },
   "outputs": [],
   "source": [
    "class PyTorchify:\n",
    "    \"\"\" Moves everything to PyTorch \"\"\"\n",
    "    def __call__(self, batch):\n",
    "        for k, v in batch.items():\n",
    "            batch[k] = torch.FloatTensor(v).to(DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can create our PPO runner! This is our pipeline of data collecting and generating mini-batches for our trainer!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-16T18:41:18.944147Z",
     "start_time": "2020-09-16T18:41:18.930156Z"
    }
   },
   "outputs": [],
   "source": [
    "def make_ppo_sampler(env, policy, num_runner_steps=2048, gamma=0.99, lambda_=0.95, num_epochs=10, num_minibatches=32):\n",
    "    \"\"\" Creates runner for PPO algorithm. \"\"\"\n",
    "    runner_transforms = [AsArray(), GAE(policy, gamma=gamma, lambda_=lambda_)]\n",
    "    runner = EnvRunner(env, policy, num_runner_steps, transforms=runner_transforms)\n",
    "\n",
    "    sampler_transforms = [NormalizeAdvantages(), PyTorchify()]\n",
    "    sampler = Sampler(runner, num_epochs=num_epochs, \n",
    "                              num_minibatches=num_minibatches,\n",
    "                              transforms=sampler_transforms)\n",
    "    return sampler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next cell you will need to implement Proximal Policy Optimization algorithm itself. The algorithm\n",
    "modifies the typical policy gradient loss in the following way:\n",
    "\n",
    "$$\n",
    "J_{\\pi}(s, a) = \\frac{\\pi_\\theta(a|s)}{\\pi_\\theta^{\\text{old}}(a|s)} \\cdot A^{\\mathrm{GAE}(\\gamma,\\lambda)}(s, a)\n",
    "$$\n",
    "\n",
    "$$\n",
    "J_{\\pi}^{\\text{clipped}}(s, a) = \\mathrm{clip}\\left(\n",
    "\\frac{\\pi_\\theta(a|s)}{\\pi_{\\theta^{\\text{old}}}(a|s)},\n",
    "1 - \\text{cliprange}, 1 + \\text{cliprange}\\right)\\cdot A^{\\mathrm{GAE(\\gamma, \\lambda)}}(s)\\\\\n",
    "$$\n",
    "\n",
    "$$\n",
    "L_{\\text{policy}} = -\\frac{1}{T}\\sum_{l=0}^{T-1}\\min\\left(J_\\pi(s_{t + l}, a_{t + l}), J_{\\pi}^{\\text{clipped}}(s_{t + l}, a_{t + l})\\right).\n",
    "$$\n",
    "\n",
    "The value loss is also modified:\n",
    "\n",
    "$$\n",
    "L_{V}^{\\text{clipped}} = \\frac{1}{T}\\sum_{l=0}^{T-1} \\max(l^{simple}(s_{t + l}), l^{clipped}(s_{t + l})),\n",
    "$$\n",
    "where $l^{simple}$ is your standard critic loss\n",
    "$$\n",
    "l^{simple}(s_{t + l}) = [V_\\theta(s_{t+l}) - \\hat{V}(s_{t + l})]^2\n",
    "$$\n",
    "\n",
    "and $l^{clipped}$ is a clipped version that limits large changes of the value function:\n",
    "$$\n",
    "l^{clipped}(s_{t + l}) = [\n",
    "V_{\\theta^{\\text{old}}}(s_{t+l}) +\n",
    "\\text{clip}\\left(\n",
    "V_\\theta(s_{t+l}) - V_{\\theta^\\text{old}}(s_{t+l}),\n",
    "-\\text{cliprange}, \\text{cliprange}\n",
    "\\right) - \\hat{V}(s_{t + l})] ^ 2\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-16T20:08:50.200496Z",
     "start_time": "2020-09-16T20:08:50.172513Z"
    }
   },
   "outputs": [],
   "source": [
    "from torch.nn.utils import clip_grad_norm_\n",
    "\n",
    "class PPO:\n",
    "    def __init__(self, policy, optimizer, sampler, cliprange=0.2, value_loss_coef=0.25, max_grad_norm=0.5):\n",
    "        self.policy = policy\n",
    "        self.optimizer = optimizer\n",
    "        self.sampler = sampler\n",
    "        self.cliprange = cliprange\n",
    "        self.value_loss_coef = value_loss_coef\n",
    "        self.max_grad_norm = max_grad_norm\n",
    "        self.iteration = 0\n",
    "        \n",
    "    def write(self, name, val):\n",
    "        \"\"\" For logging purposes \"\"\"\n",
    "        self.sampler.runner.write(name, val, self.iteration)\n",
    "\n",
    "    def policy_loss(self, batch, act):\n",
    "        \"\"\"\n",
    "        Computes and returns policy loss on a given minibatch.\n",
    "        input:\n",
    "            batch - dict from sampler, containing:\n",
    "                'advantages' - advantage estimation, tensor, (batch_size)\n",
    "                'actions' - actions selected in real trajectory, (batch_size)\n",
    "                'log_probs' - probabilities of actions from policy used to collect this trajectory, (batch_size)\n",
    "            act - dict from your current policy, containing:\n",
    "                'distribution' - MultivariateNormal, (batch_size x actions_dim)\n",
    "        output:\n",
    "            policy loss - torch scalar\n",
    "        \"\"\"    \n",
    "        <YOUR CODE>\n",
    "        \n",
    "        # additional logs: entropy, fraction of samples that were clipped, max ratio\n",
    "        self.write('additional/entropy', <>)\n",
    "        self.write('additional/policy_loss_clipped_fraction', <>)\n",
    "        self.write('additional/max_ratio', <>)\n",
    "\n",
    "    def value_loss(self, batch, act):\n",
    "        \"\"\"\n",
    "        Computes and returns policy loss on a given minibatch.\n",
    "        input:\n",
    "            batch - dict from sampler, containing:\n",
    "                'value_targets' - computed targets for critic, (batch_size)\n",
    "                'values' - critic estimation from network that generated trajectory, (batch_size)\n",
    "            act - dict from your current policy, containing:\n",
    "                'values' - current critic estimation, tensor, (batch_size)\n",
    "        output:\n",
    "            critic loss - torch scalar\n",
    "        \"\"\"\n",
    "        assert batch['value_targets'].shape == act['values'].shape, \\\n",
    "               \"Danger: your values and value targets have different shape. Watch your broadcasting!\"\n",
    "        \n",
    "        <YOUR CODE>\n",
    "        \n",
    "        # additional logs: average value predictions, fraction of samples that were clipped\n",
    "        self.write('additional/value_predictions', <>)\n",
    "        self.write('additional/value_loss_clipped_fraction', <>)\n",
    "\n",
    "    def loss(self, batch):\n",
    "        \"\"\"Computes loss for current batch\"\"\"\n",
    "        \n",
    "        # let's run our current policy on this batch \n",
    "        act = self.policy.act(batch[\"observations\"], training=True)\n",
    "        \n",
    "        # compute losses\n",
    "        # note that we don't need entropy regularization for this env.\n",
    "        <YOUR CODE>      \n",
    "                \n",
    "        # log all losses\n",
    "        self.write('losses', {\n",
    "            'policy loss': <>,\n",
    "            'critic loss': <>\n",
    "        })\n",
    "        \n",
    "        # Return scalar loss        \n",
    "        return <>\n",
    "\n",
    "    def step(self, batch):\n",
    "        \"\"\" Computes the loss function and performs a single gradient step for this batch. \"\"\"\n",
    "        <YOUR CODE>\n",
    "        \n",
    "        # do not forget to clip gradients using self.max_grad_norm\n",
    "        # and log gradient norm\n",
    "        self.write('gradient norm', <>)\n",
    "        \n",
    "        # this is for logging\n",
    "        self.iteration += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now everything is ready to do training. In **one million of interactions** it should be possible to \n",
    "achieve the average raw reward over last 100 episodes more than 1500. **Your goal is to reach 1000**.\n",
    "\n",
    "For optimization it is suggested to use Adam optimizer with learning rate 3e-4 and epsilon 1e-5.\n",
    "\n",
    "**Notes**:\n",
    "* reward should rise quickly in this environment. If it is stuck, something went wrong.\n",
    "* you can linearly decay learning rate if you face instabilities.\n",
    "* we wrote additional logs with respect to *number of network updates* since we perform many updates after each data collection stage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-16T18:41:21.867666Z",
     "start_time": "2020-09-16T18:41:21.849675Z"
    }
   },
   "outputs": [],
   "source": [
    "model = <init your model>\n",
    "policy = Policy(model)\n",
    "sampler = make_ppo_sampler(env, policy)\n",
    "\n",
    "optimizer = <choose your fighter>\n",
    "ppo = PPO(policy, optimizer, sampler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-16T18:32:27.454039Z",
     "start_time": "2020-09-16T16:06:27.078608Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "<YOUR CODE>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** remember that we learned not only the weights of policy model, but also statistics for input normalization, so to save our results we need to store them too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-16T18:40:01.398223Z",
     "start_time": "2020-09-16T18:40:01.364261Z"
    }
   },
   "outputs": [],
   "source": [
    "# save your model just in case\n",
    "\n",
    "def save(model, env, name):\n",
    "    torch.save(model.state_dict(), name)\n",
    "    np.save(name + \"_mean_ob\", env.obs_rmv.mean)\n",
    "    np.save(name + \"_var_ob\", env.obs_rmv.var)\n",
    "    np.save(name + \"_count_ob\", env.obs_rmv.count)\n",
    "    \n",
    "save(model, env, \"PPO\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-16T18:41:43.033223Z",
     "start_time": "2020-09-16T18:41:43.010236Z"
    }
   },
   "outputs": [],
   "source": [
    "# loading the model\n",
    "\n",
    "def load(model, env, name):\n",
    "    model.load_state_dict(torch.load(name))\n",
    "    env.obs_rmv.mean = np.load(name + \"_mean_ob.npy\")\n",
    "    env.obs_rmv.var = np.load(name + \"_var_ob.npy\")\n",
    "    env.obs_rmv.count = np.load(name + \"_count_ob.npy\")\n",
    "    \n",
    "load(model, env, \"PPO\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-16T18:41:47.560269Z",
     "start_time": "2020-09-16T18:41:47.546277Z"
    }
   },
   "outputs": [],
   "source": [
    "def evaluate(env, policy, n_games=1, t_max=1000):\n",
    "    '''\n",
    "    Plays n_games and returns rewards and rendered games\n",
    "    '''\n",
    "    rewards = []\n",
    "    \n",
    "    for _ in range(n_games):\n",
    "        s = env.reset()\n",
    "        \n",
    "        R = 0\n",
    "        for _ in range(t_max):\n",
    "            action = policy.act(np.array([s]))[\"actions\"][0]\n",
    "            \n",
    "            s, _, done, info = env.step(action)\n",
    "            \n",
    "            # remember that we used a wrapper that normalizes reward\n",
    "            # original reward per step comes here\n",
    "            R += info[\"original reward\"]\n",
    "                \n",
    "            if done:\n",
    "                break\n",
    "\n",
    "        rewards.append(R)\n",
    "    return np.array(rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-16T18:38:45.130920Z",
     "start_time": "2020-09-16T18:38:13.090472Z"
    }
   },
   "outputs": [],
   "source": [
    "# evaluation will take some time!\n",
    "sessions = evaluate(env, policy, n_games=20)\n",
    "score = sessions.mean()\n",
    "print(f\"Your score: {score}\")\n",
    "\n",
    "assert score >= 1000, \"Needs more training?\"\n",
    "print(\"Well done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Record"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-16T18:41:51.395874Z",
     "start_time": "2020-09-16T18:41:51.377885Z"
    }
   },
   "outputs": [],
   "source": [
    "# let's hope this will work\n",
    "# don't forget to pray\n",
    "env = gym.wrappers.Monitor(env, directory=\"videos\", force=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-16T18:42:14.863347Z",
     "start_time": "2020-09-16T18:42:06.555578Z"
    }
   },
   "outputs": [],
   "source": [
    "# record sessions\n",
    "# note that t_max is 300, so collected reward will be smaller than 1000\n",
    "evaluate(env, policy, n_games=1, t_max=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-16T18:43:19.559507Z",
     "start_time": "2020-09-16T18:43:19.522533Z"
    }
   },
   "outputs": [],
   "source": [
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
