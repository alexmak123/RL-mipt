{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Реализация алгоритма Advantage-Actor Critic (A2C) (вплоть до 10 баллов)\n",
    "\n",
    "#### дедлайн задания: 3 мая, 23:59 GMT+3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Работа выполнена: Макарчук Алексей Игоревич Б05-903в"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В данной работе Вам предстоит реализовать алгоритм `Advantage Actor Critic`, обучаемый на батче из сред `Atari 2600`, работающих параллельно.\n",
    "\n",
    "Для начала будут использованы обёртки сред, реализованные в файле `atari_wrappers.py`. Эти обёртки предварительно обрабатывают наблюдения (производят преобразования размера, цвета фрейма, взятия максимума между фреймами, пропускают часть фреймов и сводят несколько фреймов в один большой) и вознаграждения. Некоторые обёртки помогают автоматически перезапустить среду и присвоить переменной `done` значение `True` в случае смерти агента. Файл `env_batch.py` включает в себя реализацию класса `ParallelEnvBatch`, позволяющего запускать несколько сред параллельно. Для создания (инициализации) среды можно воспользоваться функцией `nature_dqn_env`. Обратите внимание, что в случае использования `PyTorch` (https://pytorch.org/) без `tensorboardX` (https://github.com/lanpa/tensorboardX) потребуется самостоятельно реализовать обёртку среды, которая будет логрировать **исходные** суммарные награды, которые *исходная* среда возвращает, и переопределить реализацию функции `nature_dqn_env`. То есть настоятельно рекомендуется применить `tensorboardX`.\n",
    "\n",
    "Псевдокод алгоритма `Advantage Actor Critic (A2C)` приведён в **Разделе 5.2.5 (Алгоритм 20)** конспекта лекций: https://arxiv.org/pdf/2201.09746.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Скрипты в данной работе используют Python версию библиотеки [OpenCV](https://pypi.org/project/opencv-python/). Для корректной работы ноутбука возможно понадобится доустановить библиотеку `gym`, запустив один из следующих наборов команд (пример приведён для Unix-подобных систем):\n",
    "```\n",
    "sudo apt-get install -y xvfb x11-utils ffmpeg python-opengl\n",
    "pip install --upgrade pyglet pyvirtualdisplay opencv-python tqdm numpy gym[atari]==0.18.3\n",
    "# wget https://bit.ly/2FMJP5K -O setup.py && bash setup.py # раскомментировать для Google Colab\n",
    "```\n",
    "или\n",
    "```\n",
    "sudo apt-get install -y xvfb x11-utils ffmpeg python-opengl\n",
    "pip install --upgrade pyglet pyvirtualdisplay opencv-python tqdm numpy gym[all]==0.18.3\n",
    "# wget https://bit.ly/2FMJP5K -O setup.py && bash setup.py # раскомментировать для Google Colab\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# XVFB будет запущен в случае исполнения на сервере\n",
    "import os\n",
    "\n",
    "\n",
    "if type(os.environ.get(\"DISPLAY\")) is not str or len(os.environ.get(\"DISPLAY\")) == 0:\n",
    "    !bash ../xvfb start\n",
    "    os.environ['DISPLAY'] = ':1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from atari_wrappers import nature_dqn_env\n",
    "\n",
    "\n",
    "env = nature_dqn_env(\"SpaceInvadersNoFrameskip-v4\", nenvs=8) # nenvs -- количество параллельно запущенных сред\n",
    "                                                             # данный параметр можно варьировать для баланса\n",
    "                                                             # производительность итерации/надёжность Монте-Карло оценок\n",
    "                                                             # помните: при уменьшении nenvs, возможно, придётся\n",
    "                                                             # увеличить количество итераций оптимизации\n",
    "n_actions = env.action_space.spaces[0].n\n",
    "obs = env.reset()\n",
    "assert obs.shape == (8, 84, 84, 4)\n",
    "assert obs.dtype == np.float64"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В случае возникновения исключения `ROM is missing for space_invaders` выше рекомендуется установить ROM-ы согласно инструкции по адресу: https://github.com/openai/atari-py#roms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Следующим шагом будет реализация модели, которая выводит логиты для категориального распределения на действия и оценку на значения $V$-функции ценности. Рекомендуется использовать архитектуру модели, представленной в публикации в журнале [Nature](https://web.stanford.edu/class/psych209/Readings/MnihEtAlHassibis15NatureControlDeepRL.pdf) со следующей модификацией: вместо одного выходного слоя нужно сделать два слоя, принимающих в качестве входа выход предшествующего скрытого слоя. **Обратите внимание**, данная модель отличается от модели, предложенной в домашней работе по DQN. Рекомендуется использовать ортогональную инициализацию с параметром $\\sqrt{2}$ для ядер свёрток и инициализировать смещения нулями."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.optim as opt\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "\n",
    "def conv2d_size_out(size, kernel_size, stride):\n",
    "    \"\"\"\n",
    "    общий принцип использования данной функции:\n",
    "    cur_layer_img_w = conv2d_size_out(cur_layer_img_w, kernel_size, stride)\n",
    "    cur_layer_img_h = conv2d_size_out(cur_layer_img_h, kernel_size, stride)\n",
    "    для вычисления размерности входа полносвязного слоя\n",
    "    \"\"\"\n",
    "    return (size - (kernel_size - 1) - 1) // stride  + 1\n",
    "\n",
    "\n",
    "class Flatten(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return x.reshape(x.size(0), -1)\n",
    "\n",
    "\n",
    "class NatureModel(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self, bsize, height, width, nchannels, n_actions, device):\n",
    "        super().__init__()\n",
    "        self.device = device\n",
    "        \n",
    "        self.conv1 = torch.nn.Conv2d(in_channels=nchannels, out_channels=32, kernel_size=8, stride=4)\n",
    "        self.relu1 = torch.nn.ReLU()\n",
    "        cur_h, cur_w = conv2d_size_out(height, 8, 4), conv2d_size_out(width, 8, 4)\n",
    "        self.conv2 = torch.nn.Conv2d(in_channels=32, out_channels=64, kernel_size=4, stride=2)\n",
    "        self.relu2 = torch.nn.ReLU()\n",
    "        cur_h, cur_w = conv2d_size_out(cur_h, 4, 2), conv2d_size_out(cur_w, 4, 2)\n",
    "        self.conv3 = torch.nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, stride=1)\n",
    "        self.relu3 = torch.nn.ReLU()\n",
    "        cur_h, cur_w = conv2d_size_out(cur_h, 3, 1), conv2d_size_out(cur_w, 3, 1)\n",
    "        self.flatten = Flatten()\n",
    "        self.fc1 = torch.nn.Linear(in_features=64 * cur_h * cur_w, out_features=512)\n",
    "        self.relu4 = torch.nn.ReLU()\n",
    "        \n",
    "        self.fc_logits = torch.nn.Linear(in_features=512, out_features=n_actions)\n",
    "        self.fc_values = torch.nn.Linear(in_features=512, out_features=1)\n",
    "        \n",
    "        self._reset_parameters()\n",
    "    \n",
    "    def _reset_parameters(self):\n",
    "        torch.nn.init.orthogonal_(self.conv1.weight.data, gain=np.sqrt(2))\n",
    "        self.conv1.bias.data.fill_(0.0)\n",
    "        \n",
    "        torch.nn.init.orthogonal_(self.conv2.weight.data, gain=np.sqrt(2))\n",
    "        self.conv2.bias.data.fill_(0.0)\n",
    "        \n",
    "        torch.nn.init.orthogonal_(self.conv3.weight.data, gain=np.sqrt(2))\n",
    "        self.conv3.bias.data.fill_(0.0)\n",
    "        \n",
    "        torch.nn.init.orthogonal_(self.fc1.weight.data, gain=np.sqrt(2))\n",
    "        self.fc1.bias.data.fill_(0.0)\n",
    "        \n",
    "        torch.nn.init.orthogonal_(self.fc_logits.weight.data, gain=np.sqrt(2))\n",
    "        self.fc_logits.bias.data.fill_(0.0)\n",
    "        \n",
    "        torch.nn.init.orthogonal_(self.fc_values.weight.data, gain=np.sqrt(2))\n",
    "        self.fc_values.bias.data.fill_(0.0)\n",
    "    \n",
    "    def forward(self, state):\n",
    "        x = torch.tensor(state, device=self.device, dtype=torch.float).permute(0, 3, 1, 2)\n",
    "        \n",
    "        x = self.conv1(x)\n",
    "        x = self.relu1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.relu2(x)\n",
    "        x = self.conv3(x)\n",
    "        x = self.relu3(x)\n",
    "        x = self.flatten(x)\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu4(x)\n",
    "        \n",
    "        logits = self.fc_logits(x)\n",
    "        value = self.fc_values(x)\n",
    "        \n",
    "        x.cpu()\n",
    "        del x\n",
    "        \n",
    "        return logits, value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вам также потребуется определить и использовать политику, которая будет использовать модель выше. В то время как модель вычисляет логиты для всех действий сразу и оценку функции ценности, политика будет сэмплировать действия, а также будет вычислять их логарифм правдоподобия. Метод `Policy.act` должен возвращать словарь всех массивов, требуемых для взаимодействия со средой и обучения модели. Обратите внимание, что действия должны быть формата `Numpy.ndarray`, в то время как другие тензоры должны быть в формате, определяемом библиотекой глубокого обучения (`Torch.tensor`, например)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.distributions import Categorical\n",
    "\n",
    "\n",
    "class Policy:\n",
    "    def __init__(self, model):\n",
    "        self.model = model\n",
    "\n",
    "    def act(self, inputs):\n",
    "        <Реализуйте политику посредством прямого прохода модели, сэмплирования действий и вычисления их логарифмов правдоподобия>\n",
    "        # Должен возвращать dict с ключами ['actions', 'logits', 'log_probs', 'values'].\n",
    "    \n",
    "    def reset(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Далее требуется передать среду и политику в исполнитель `EnvRunner`, который собирает частичные траектории из среды. Класс `EnvRunner` уже реализован за Вас."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from runners import EnvRunner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Данный исполнитель взаимодействует со средой заданное количество шагов и возвращает словарь, содержащий ключи:\n",
    "\n",
    "* 'observations' \n",
    "* 'rewards' \n",
    "* 'resets'\n",
    "* 'actions'\n",
    "* и другие ключи, определённые в `Policy`\n",
    "\n",
    "по каждому из этих ключей содержится Python `list` соответствующих результатов взаимодействий со средой указанной длины $T$ &mdash; размера частичной траектории."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для того чтобы обучить часть модели, которая предсказывает ценности состояний, требуется вычислить целевые значения ценностей. В инстанцию класса `EnvRunner` можно подать при создании по аргументу `transforms` список вызываемых объектов (\"функций\"), которые последовательно будут применяться к частичным траекториям после сбора самих траекторий. Следовательно, требуется реализовать и использовать вызываемый (с определённым методом `__call__`) класс `ComputeValueTargets`. Формула для вычисления целевых значений ценности простая:\n",
    "\n",
    "$$\n",
    "\\hat v(s_t) = \\sum_{t'=0}^{T - 1}\\gamma^{t'}r_{t+t'} + \\gamma^T \\hat{v}(s_{t+T}),\n",
    "$$\n",
    "\n",
    "Однако, не забудьте в реализации использовать `trajectory['resets']` флаги для проверки того, следует ли добавить целевые значения ценности на следующем шаге при вычислении целевых значений ценности на текущем шаге. У вас также имеется доступ к `trajectory['state']['latest_observation']` для получения последнего наблюдения в частичной траектории &mdash; $s_{t+T}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ComputeValueTargets:\n",
    "    def __init__(self, gamma=0.99, device=torch.device('cuda:0')):\n",
    "        self.gamma = gamma\n",
    "        self.device = device\n",
    "\n",
    "    def __call__(self, trajectory):\n",
    "        # Данный метод должен модифицировать траекторию trajectory на месте через добавление\n",
    "        # итеративно заполненного списка по ключу 'value_targets'.\n",
    "        <Вычислите целевые значения ценности для данных частичных траекторий>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "После вычисления целевых значений ценности требуется преобразовать списки результатов взаимодействия со средой в тензоры с первой компонентой размерности `batch_size`, равной призведению `T * nenvs`, то есть требуется свести в одну компоненту первые две компоненты размерности."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "class MergeTimeBatch:\n",
    "    \"\"\" Сращивает первые две оси, обычно отвечающие за время и за инстанцию среды, соответственно. \"\"\"\n",
    "    def __call__(self, trajectory):\n",
    "        # Модификация траектории на месте. \n",
    "        <TODO: реализовать>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 617 # на своё усмотрение можно выбрать другой seed\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.backends.cudnn.benchmark = True\n",
    "\n",
    "\n",
    "model = NatureModel(*obs.shape, n_actions, device)\n",
    "model.to(device)\n",
    "policy = Policy(model)\n",
    "runner = EnvRunner(env, policy, nsteps=5, # nsteps -- длина частичной траектории\n",
    "                                          # уменьшение nsteps может привести к вынужденному увеличению\n",
    "                                          # количества итераций оптимизации\n",
    "                   transforms=[ComputeValueTargets(device=device),\n",
    "                               MergeTimeBatch()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Настало время реализовать сам алгоритм Advantage-Actor Critic (A2C). Его псевдокод можно посмотреть в [конспектах лекций (Раздел 5.2.5)](https://arxiv.org/pdf/2201.09746.pdf), в публикции [Mnih et al. 2016](https://arxiv.org/abs/1602.01783) и в [лекции](https://www.youtube.com/watch?v=Tol_jw5hWnI&list=PLkFD6_40KJIxJMR-j5A1mkxK26gh_qg37&index=20) Сергея Левина."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class A2C:\n",
    "    def __init__(self,\n",
    "               policy,\n",
    "               optimizer,\n",
    "               value_loss_coef=0.25,\n",
    "               entropy_coef=0.01,\n",
    "               max_grad_norm=0.5):\n",
    "        self.policy = policy\n",
    "        self.optimizer = optimizer\n",
    "        self.value_loss_coef = value_loss_coef\n",
    "        self.entropy_coef = entropy_coef\n",
    "        self.max_grad_norm = max_grad_norm\n",
    "\n",
    "    def policy_loss(self, trajectory):\n",
    "        # Тут нужно вычислить Advantages. \n",
    "        <TODO: реализовать>\n",
    "\n",
    "    def value_loss(self, trajectory):\n",
    "        <TODO: реализовать>\n",
    "\n",
    "    def loss(self, trajectory):\n",
    "        <TODO: реализовать>\n",
    "\n",
    "    def step(self, trajectory):\n",
    "        <TODO: реализовать>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь можно непосредственно обучить Вашу модель. С разумно подобранными гиперпараметрами обучение на одной GTX1080 на протяжении 10 миллионов шагов суммарно со всех батчированных сред (что переводится примерно в 5 часов работы) должно быть возможно достигнуть *среднюю исходную награду за 100 последних эпизодов* (значение переменной в `Tensorboard` по ключу `reward_mean_100`, усреднение берётся по 100 последним эпизодам в каждой среде в батче) **не меньше 600**. Это и будет считаться успешным результатом работы алгоритма `A2C`.\n",
    "\n",
    "Вам так же, по возможности, рекомендуется отобразить данную величину относительно `runner.step_var` &mdash; количества взаимодействий со всеми средами. Также очень рекомендуется предоставить графики следующих показателей (полезно для отладки кода):\n",
    "* [Коэффициент детерминации](https://en.wikipedia.org/wiki/Coefficient_of_determination) между целевыми значениями ценности и их предсказаниями\n",
    "* Энтропия политики $\\pi$\n",
    "* Функция потерь ценности (Value loss)\n",
    "* Функция потерь политики (Policy loss)\n",
    "* Целевые значения ценности (Value targets)\n",
    "* Предсказания значений ценности (Value predictions)\n",
    "* Норма градиента\n",
    "* Advantages\n",
    "* Общая функция потерь (A2C loss)\n",
    "\n",
    "В качестве оптимизатора рекомендуется взять метод [RMSProp](https://pytorch.org/docs/stable/generated/torch.optim.RMSprop.html) с [линейным убыванием шага](https://pytorch.org/docs/stable/generated/torch.optim.lr_scheduler.LambdaLR.html), начиная с 7e-4 до 0, константой сглаживания (alpha в PyTorch и decay в TensorFlow), равной 0.99 и epsilon, равным 1e-5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a2c = <TODO: создать инстанцию класса A2C>\n",
    "\n",
    "<TODO: реализовать цикл оптимизации политики согласно алгоритму A2C>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_env = nature_dqn_env(\"SpaceInvadersNoFrameskip-v4\", nenvs=None, clip_reward=False, summaries=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_lives = 5\n",
    "\n",
    "\n",
    "def evaluate(env, agent, n_games=1, greedy=False, t_max=10000):\n",
    "    \"\"\" Играем n_games игр до конца. В случае жадной политики, выбираем действия как argmax(qvalues). Возвращаем среднюю награду. \"\"\"\n",
    "    rewards = []\n",
    "    for _ in range(n_games):\n",
    "        s = env.reset()\n",
    "        reward = 0\n",
    "        for _ in range(t_max):\n",
    "            output = agent.act([s])\n",
    "            action = output['logits'].argmax(dim=-1).item() if greedy else output['actions'][0]\n",
    "            s, r, done, _ = env.step(action)\n",
    "            reward += r\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "        rewards.append(reward)\n",
    "    return np.mean(rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym.wrappers\n",
    "\n",
    "\n",
    "env_monitor = gym.wrappers.Monitor(test_env, directory=\"videos\", force=True)\n",
    "sessions = [evaluate(env_monitor, policy, n_games=n_lives, greedy=True) for _ in range(10)]\n",
    "env_monitor.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_monitor = gym.wrappers.Monitor(test_env, directory=\"stochasticvideos\", force=True)\n",
    "sessions = [evaluate(env_monitor, policy, n_games=n_lives, greedy=False) for _ in range(10)]\n",
    "env_monitor.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import HTML\n",
    "from base64 import b64encode\n",
    "\n",
    "\n",
    "video_names = list(\n",
    "    filter(lambda s: s.endswith(\".mp4\"), os.listdir(\"./videos\")))\n",
    "mp4 = open(\"./videos/\" + video_names[-1], \"rb\").read()\n",
    "data_url = \"data:video/mp4;base64,\" + b64encode(mp4).decode()\n",
    "\n",
    "HTML(\"\"\"\n",
    "<video width=\"640\" height=\"480\" controls>\n",
    "  <source src=\"{}\" type=\"video/mp4\">\n",
    "</video>\n",
    "\"\"\".format(data_url))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_names = list(\n",
    "    filter(lambda s: s.endswith(\".mp4\"), os.listdir(\"./stochasticvideos\")))\n",
    "mp4 = open(\"./stochasticvideos/\" + video_names[-1], \"rb\").read()\n",
    "data_url = \"data:video/mp4;base64,\" + b64encode(mp4).decode()\n",
    "\n",
    "HTML(\"\"\"\n",
    "<video width=\"640\" height=\"480\" controls>\n",
    "  <source src=\"{}\" type=\"video/mp4\">\n",
    "</video>\n",
    "\"\"\".format(data_url))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Информация об обучении"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Прикрепление скриншотов графиков обучения модели в `Tensorboard` ниже является обязательным. Для доступа к `Tensorboard` запустите из командной строки в одной директории с данным ноутбуком следующую команду:\n",
    "```\n",
    "tensorboard --logdir logs --port 6006\n",
    "```\n",
    "В результате вывод в командную строку укажет, по какому адресу можно подсоединиться к инстанции `Tensorboard`, например, по адресу `http://localhost:6006/`. Оттуда можно и сделать скриншоты, демонстрирующие результаты обучения модели. Сами скриншоты с именем файла `image_name_x.png` для удобства лучше сохранить в директорию `./img`, откуда можно легко их прикреплять в `Markdown-клетках` ниже по команде со следующей конструкцией:\n",
    "```\n",
    "<img src=./img/image_name_x.png width=640>\n",
    "```\n",
    "Тут также требуется подписать изображения и дать небольшой комментарий по каждому скриншоту, что на нём описано.\n",
    "\n",
    "**Внимание!** В случае перезапуска процедуры обучения модели рекомендуется удалить директорию `./logs` вместе с её содержимым перед непосредственным перезапуском, чтобы не испортить отображающиеся графики в `Tensorboard`.\n",
    "\n",
    "**Совет.** При работе в Google Colab можно просто скачать директорию `./logs` и уже локально запустить `Tensorboard` для снятия скриншотов. Также можно обученного агента сохранить, скачать и локально на cpu запустить для записи роликов (для этого понадобится самостоятельно прописать код сохранения и загрузки модели в ноутбук из [файла](https://pytorch.org/tutorials/beginner/saving_loading_models.html)).\n",
    "\n",
    "**Внимание!** Посылку для сдачи задания требуется оформить в виде `.zip` архива, в котором будут *данный ноутбук*, использованные для его работы *скрипты*, *директории* `./videos`, `./stochasticvideos`, `./logs` и `./img` с содержимым. Только так и не иначе!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Вставьте в данную ячейку свой ответ, подкреплённый скриншотами__"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
